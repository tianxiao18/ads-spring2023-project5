{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fc8f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbe7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = np.load('new_feature_matrix.npy')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "drop_rate = 0.5\n",
    "time_step = 5\n",
    "epochs = 200\n",
    "feature_matrix = np.load('new_feature_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "580139d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "## normalization\n",
    "def normalization(feature_matrix):\n",
    "    feature_mat = np.zeros((feature_matrix.shape[0],feature_matrix.shape[1],feature_matrix.shape[2]))\n",
    "    target = np.zeros((feature_matrix.shape[0],feature_matrix.shape[1]))\n",
    "    for i in range(feature_matrix.shape[0]):\n",
    "        feature_vector=feature_matrix[i]\n",
    "        f1 = scaler.fit_transform(feature_vector[:,0].reshape(-1,1)).reshape(-1,1)\n",
    "        f2 = scaler.fit_transform(feature_vector[:,1].reshape(-1,1)).reshape(-1,1)\n",
    "        f3 = scaler.fit_transform(feature_vector[:,2].reshape(-1,1)).reshape(-1,1)\n",
    "        f4 = scaler.fit_transform(feature_vector[:,3].reshape(-1,1)).reshape(-1,1)\n",
    "        visit_nums = scaler.fit_transform(feature_vector[:,4].reshape(-1,1)).reshape(-1,1)\n",
    "        feature_vector = np.stack([f1,f2,f3,f4,visit_nums],axis=1).reshape(feature_matrix.shape[1],feature_matrix.shape[2])\n",
    "        feature_mat[i] = feature_vector\n",
    "    return feature_mat\n",
    "\n",
    "\n",
    "## split into train, test and validation\n",
    "features = feature_matrix\n",
    "features = normalization(features)\n",
    "train_idx, test_idx, val_idx = [],[],[]\n",
    "for i in range(features.shape[0]):\n",
    "    if i%5<=2:\n",
    "        train_idx.append(i)\n",
    "    elif i%5==3:\n",
    "        val_idx.append(i)\n",
    "    else:\n",
    "        test_idx.append(i)\n",
    "\n",
    "train_features = features[train_idx,:,:]\n",
    "test_features = features[test_idx,:,:]\n",
    "val_features = features[val_idx,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9a82ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 172, 5)\n",
      "(18, 172, 5)\n",
      "(54, 172, 5)\n"
     ]
    }
   ],
   "source": [
    "print(val_features.shape)\n",
    "print(test_features.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(features, index):\n",
    "    f = features[:,index,:].cpu()\n",
    "    new = np.zeros((len(f) - time_step, time_step, 4-1))\n",
    "    target = np.zeros((len(f) - time_step, 1))\n",
    "    for i in range(len(f) - time_step):\n",
    "        new[i] = f[i:i+time_step,1:4]\n",
    "        target[i] = f[i+time_step, 4]\n",
    "    return new, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97118c13",
   "metadata": {},
   "source": [
    "# LSTM(Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e267cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_i, n_h, n_o):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(n_i, n_h)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(n_h, n_o)\n",
    "    def forward(self, input):\n",
    "        input = self.linear1(input)\n",
    "        input = self.relu(input)\n",
    "        input = F.dropout(input, p=drop_rate, training=self.training)\n",
    "        input = self.linear2(input)\n",
    "        return input\n",
    "###Convert the data to tensor \n",
    "def totensor(data):\n",
    "    return torch.Tensor(data).to(device)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.mlp = MLP(time_step*4,32,1)#input is the num of time_step\n",
    "        self.lstm1 = nn.LSTM(input_size=3,hidden_size=128,num_layers=2,batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128,hidden_size=4,num_layers=2,batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x1,x2,x3 = x.size()\n",
    "        h1, (h1_T,c1_T) = self.lstm1(x)\n",
    "        h2, (h2_T, c2_T) = self.lstm2(h1)\n",
    "        h2 = h2.reshape(x1,-1)\n",
    "        output= self.mlp(h2)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c03d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = np.inf\n",
    "mean_losses = []\n",
    "losses = pd.DataFrame(columns = ['Training Loss', 'Validating Loss','Testing Loss'])\n",
    "model = LSTM().cpu()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fc1e793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e72308df7f44078a2346d9bc0b406e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Lr: 0.00100000000000000002 |Train loss: 1.00992788|Val loss: 1.01476971\n",
      "Epoch: 002 | Lr: 0.00100000000000000002 |Train loss: 1.00992788|Val loss: 1.04643409\n",
      "Epoch: 003 | Lr: 0.00100000000000000002 |Train loss: 1.00992788|Val loss: 1.03875395\n",
      "Epoch: 004 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.00986144\n",
      "Epoch: 005 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.07259697\n",
      "Epoch: 006 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.04745652\n",
      "Epoch: 007 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.03473672\n",
      "Epoch: 008 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.02873363\n",
      "Epoch: 009 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.02114371\n",
      "Epoch: 010 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.02224891\n",
      "Epoch: 011 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.01854520\n",
      "Epoch: 012 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.02317961\n",
      "Epoch: 013 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.01731500\n",
      "Epoch: 014 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.01442896\n",
      "Epoch: 015 | Lr: 0.00100000000000000002 |Train loss: 1.00985567|Val loss: 1.01171547\n",
      "Epoch: 016 | Lr: 0.00100000000000000002 |Train loss: 0.98177976|Val loss: 0.98130612\n",
      "Epoch: 017 | Lr: 0.00100000000000000002 |Train loss: 0.98177976|Val loss: 1.01744993\n",
      "Epoch: 018 | Lr: 0.00100000000000000002 |Train loss: 0.97536557|Val loss: 0.97123204\n",
      "Epoch: 019 | Lr: 0.00100000000000000002 |Train loss: 0.95012141|Val loss: 0.94604714\n",
      "Epoch: 020 | Lr: 0.00100000000000000002 |Train loss: 0.90026643|Val loss: 0.90828776\n",
      "Epoch: 021 | Lr: 0.00100000000000000002 |Train loss: 0.90026643|Val loss: 0.92609565\n",
      "Epoch: 022 | Lr: 0.00100000000000000002 |Train loss: 0.89572640|Val loss: 0.89682983\n",
      "Epoch: 023 | Lr: 0.00100000000000000002 |Train loss: 0.88028877|Val loss: 0.87771429\n",
      "Epoch: 024 | Lr: 0.00100000000000000002 |Train loss: 0.88028877|Val loss: 0.87577793\n",
      "Epoch: 025 | Lr: 0.00100000000000000002 |Train loss: 0.86401314|Val loss: 0.86280823\n",
      "Epoch: 026 | Lr: 0.00100000000000000002 |Train loss: 0.86401314|Val loss: 0.87815724\n",
      "Epoch: 027 | Lr: 0.00100000000000000002 |Train loss: 0.85884032|Val loss: 0.87023234\n",
      "Epoch: 028 | Lr: 0.00100000000000000002 |Train loss: 0.84003023|Val loss: 0.84423402\n",
      "Epoch: 029 | Lr: 0.00100000000000000002 |Train loss: 0.84003023|Val loss: 0.84305546\n",
      "Epoch: 030 | Lr: 0.00100000000000000002 |Train loss: 0.83646544|Val loss: 0.85332170\n",
      "Epoch: 031 | Lr: 0.00100000000000000002 |Train loss: 0.83646544|Val loss: 0.86105666\n",
      "Epoch: 032 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.83193393\n",
      "Epoch: 033 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.88585148\n",
      "Epoch: 034 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.88058347\n",
      "Epoch: 035 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.85497302\n",
      "Epoch: 036 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.85507397\n",
      "Epoch: 037 | Lr: 0.00100000000000000002 |Train loss: 0.83169937|Val loss: 0.83844659\n",
      "Epoch: 038 | Lr: 0.00100000000000000002 |Train loss: 0.82695194|Val loss: 0.84217630\n",
      "Epoch: 039 | Lr: 0.00100000000000000002 |Train loss: 0.82695194|Val loss: 0.87562357\n",
      "Epoch: 040 | Lr: 0.00100000000000000002 |Train loss: 0.82695194|Val loss: 0.84407132\n",
      "Epoch: 041 | Lr: 0.00100000000000000002 |Train loss: 0.82695194|Val loss: 0.83444826\n",
      "Epoch: 042 | Lr: 0.00100000000000000002 |Train loss: 0.82640537|Val loss: 0.84671668\n",
      "Epoch: 043 | Lr: 0.00100000000000000002 |Train loss: 0.82235319|Val loss: 0.83991401\n",
      "Epoch: 044 | Lr: 0.00100000000000000002 |Train loss: 0.82235319|Val loss: 0.84420652\n",
      "Epoch: 045 | Lr: 0.00100000000000000002 |Train loss: 0.81483868|Val loss: 0.82943530\n",
      "Epoch: 046 | Lr: 0.00100000000000000002 |Train loss: 0.81483868|Val loss: 0.82835495\n",
      "Epoch: 047 | Lr: 0.00100000000000000002 |Train loss: 0.81206965|Val loss: 0.81354709\n",
      "Epoch: 048 | Lr: 0.00100000000000000002 |Train loss: 0.81206965|Val loss: 0.81672322\n",
      "Epoch: 049 | Lr: 0.00100000000000000002 |Train loss: 0.80349288|Val loss: 0.83610339\n",
      "Epoch: 050 | Lr: 0.00100000000000000002 |Train loss: 0.80310121|Val loss: 0.81146373\n",
      "Epoch: 051 | Lr: 0.00100000000000000002 |Train loss: 0.80310121|Val loss: 0.83468007\n",
      "Epoch: 052 | Lr: 0.00100000000000000002 |Train loss: 0.80310121|Val loss: 0.80174690\n",
      "Epoch: 053 | Lr: 0.00100000000000000002 |Train loss: 0.79631797|Val loss: 0.78832555\n",
      "Epoch: 054 | Lr: 0.00100000000000000002 |Train loss: 0.79631797|Val loss: 0.82042020\n",
      "Epoch: 055 | Lr: 0.00100000000000000002 |Train loss: 0.79631797|Val loss: 0.82644547\n",
      "Epoch: 056 | Lr: 0.00100000000000000002 |Train loss: 0.79290523|Val loss: 0.80339893\n",
      "Epoch: 057 | Lr: 0.00100000000000000002 |Train loss: 0.79290523|Val loss: 0.81503012\n",
      "Epoch: 058 | Lr: 0.00100000000000000002 |Train loss: 0.79290523|Val loss: 0.80872017\n",
      "Epoch: 059 | Lr: 0.00100000000000000002 |Train loss: 0.78533758|Val loss: 0.78608109\n",
      "Epoch: 060 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.77848479\n",
      "Epoch: 061 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.81533317\n",
      "Epoch: 062 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.82448409\n",
      "Epoch: 063 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.79408885\n",
      "Epoch: 064 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.78410242\n",
      "Epoch: 065 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.78778307\n",
      "Epoch: 066 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.77017239\n",
      "Epoch: 067 | Lr: 0.00100000000000000002 |Train loss: 0.77352142|Val loss: 0.79227776\n",
      "Epoch: 068 | Lr: 0.00100000000000000002 |Train loss: 0.76360507|Val loss: 0.76921241\n",
      "Epoch: 069 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.74805823\n",
      "Epoch: 070 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.77742528\n",
      "Epoch: 071 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.77980867\n",
      "Epoch: 072 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.90719010\n",
      "Epoch: 073 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.78565846\n",
      "Epoch: 074 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.78118905\n",
      "Epoch: 075 | Lr: 0.00100000000000000002 |Train loss: 0.73725443|Val loss: 0.75474433\n",
      "Epoch: 076 | Lr: 0.00100000000000000002 |Train loss: 0.72859609|Val loss: 0.75710350\n",
      "Epoch: 077 | Lr: 0.00100000000000000002 |Train loss: 0.72859609|Val loss: 0.74857907\n",
      "Epoch: 078 | Lr: 0.00100000000000000002 |Train loss: 0.72607426|Val loss: 0.70775582\n",
      "Epoch: 079 | Lr: 0.00100000000000000002 |Train loss: 0.72607426|Val loss: 0.76188358\n",
      "Epoch: 080 | Lr: 0.00100000000000000002 |Train loss: 0.72607426|Val loss: 0.75411340\n",
      "Epoch: 081 | Lr: 0.00100000000000000002 |Train loss: 0.72607426|Val loss: 0.76349542\n",
      "Epoch: 082 | Lr: 0.00100000000000000002 |Train loss: 0.70277733|Val loss: 0.72719561\n",
      "Epoch: 083 | Lr: 0.00100000000000000002 |Train loss: 0.70277733|Val loss: 0.71177123\n",
      "Epoch: 084 | Lr: 0.00100000000000000002 |Train loss: 0.70277733|Val loss: 0.84040868\n",
      "Epoch: 085 | Lr: 0.00100000000000000002 |Train loss: 0.70277733|Val loss: 0.73354517\n",
      "Epoch: 086 | Lr: 0.00100000000000000002 |Train loss: 0.70277733|Val loss: 0.71195696\n",
      "Epoch: 087 | Lr: 0.00100000000000000002 |Train loss: 0.70195247|Val loss: 0.70095941\n",
      "Epoch: 088 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.68191218\n",
      "Epoch: 089 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.69442447\n",
      "Epoch: 090 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.72300605\n",
      "Epoch: 091 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.76190292\n",
      "Epoch: 092 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.78227553\n",
      "Epoch: 093 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.69543377\n",
      "Epoch: 094 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.69248435\n",
      "Epoch: 095 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.70692648\n",
      "Epoch: 096 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.68460669\n",
      "Epoch: 097 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.69500976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 098 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.75987217\n",
      "Epoch: 099 | Lr: 0.00100000000000000002 |Train loss: 0.68074998|Val loss: 0.70437937\n",
      "Epoch: 100 | Lr: 0.00100000000000000002 |Train loss: 0.67999685|Val loss: 0.67151921\n",
      "Epoch: 101 | Lr: 0.00100000000000000002 |Train loss: 0.67999685|Val loss: 0.69723768\n",
      "Epoch: 102 | Lr: 0.00100000000000000002 |Train loss: 0.66885669|Val loss: 0.67123852\n",
      "Epoch: 103 | Lr: 0.00100000000000000002 |Train loss: 0.66885669|Val loss: 0.72929063\n",
      "Epoch: 104 | Lr: 0.00100000000000000002 |Train loss: 0.66885669|Val loss: 0.67740875\n",
      "Epoch: 105 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.66846351\n",
      "Epoch: 106 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.68190122\n",
      "Epoch: 107 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.70889260\n",
      "Epoch: 108 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.77175566\n",
      "Epoch: 109 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.73919181\n",
      "Epoch: 110 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.69631257\n",
      "Epoch: 111 | Lr: 0.00100000000000000002 |Train loss: 0.66808195|Val loss: 0.69501646\n",
      "Epoch: 112 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.64878885\n",
      "Epoch: 113 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.69489235\n",
      "Epoch: 114 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.73573050\n",
      "Epoch: 115 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.69038511\n",
      "Epoch: 116 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.71352857\n",
      "Epoch: 117 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.68720702\n",
      "Epoch: 118 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.66533439\n",
      "Epoch: 119 | Lr: 0.00100000000000000002 |Train loss: 0.66037320|Val loss: 0.64910104\n",
      "Epoch: 120 | Lr: 0.00100000000000000002 |Train loss: 0.65885318|Val loss: 0.64670576\n",
      "Epoch: 121 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.65980747\n",
      "Epoch: 122 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.68532376\n",
      "Epoch: 123 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.75417709\n",
      "Epoch: 124 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.70319983\n",
      "Epoch: 125 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.69763414\n",
      "Epoch: 126 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.66883247\n",
      "Epoch: 127 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.64494874\n",
      "Epoch: 128 | Lr: 0.00100000000000000002 |Train loss: 0.65796346|Val loss: 0.64984938\n",
      "Epoch: 129 | Lr: 0.00100000000000000002 |Train loss: 0.65683525|Val loss: 0.66487082\n",
      "Epoch: 130 | Lr: 0.00100000000000000002 |Train loss: 0.65683525|Val loss: 0.66407248\n",
      "Epoch: 131 | Lr: 0.00100000000000000002 |Train loss: 0.64465767|Val loss: 0.64464565\n",
      "Epoch: 132 | Lr: 0.00100000000000000002 |Train loss: 0.64465767|Val loss: 0.64835287\n",
      "Epoch: 133 | Lr: 0.00100000000000000002 |Train loss: 0.64465767|Val loss: 0.75732555\n",
      "Epoch: 134 | Lr: 0.00100000000000000002 |Train loss: 0.64465767|Val loss: 0.65775800\n",
      "Epoch: 135 | Lr: 0.00100000000000000002 |Train loss: 0.64465767|Val loss: 0.65619784\n",
      "Epoch: 136 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.62574147\n",
      "Epoch: 137 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.63444528\n",
      "Epoch: 138 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.73033528\n",
      "Epoch: 139 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.72711386\n",
      "Epoch: 140 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.74432293\n",
      "Epoch: 141 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.67196877\n",
      "Epoch: 142 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.63799561\n",
      "Epoch: 143 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.63466006\n",
      "Epoch: 144 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.65098403\n",
      "Epoch: 145 | Lr: 0.00100000000000000002 |Train loss: 0.63535970|Val loss: 0.63146995\n",
      "Epoch: 146 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.61104952\n",
      "Epoch: 147 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.62127881\n",
      "Epoch: 148 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.70083237\n",
      "Epoch: 149 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.73435921\n",
      "Epoch: 150 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.65537821\n",
      "Epoch: 151 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.64913470\n",
      "Epoch: 152 | Lr: 0.00100000000000000002 |Train loss: 0.62596734|Val loss: 0.71047925\n",
      "Epoch: 153 | Lr: 0.00100000000000000002 |Train loss: 0.61920494|Val loss: 0.61936308\n",
      "Epoch: 154 | Lr: 0.00100000000000000002 |Train loss: 0.61303143|Val loss: 0.59351761\n",
      "Epoch: 155 | Lr: 0.00100000000000000002 |Train loss: 0.60371310|Val loss: 0.61360905\n",
      "Epoch: 156 | Lr: 0.00100000000000000002 |Train loss: 0.59965945|Val loss: 0.59695231\n",
      "Epoch: 157 | Lr: 0.00100000000000000002 |Train loss: 0.59682012|Val loss: 0.59049793\n",
      "Epoch: 158 | Lr: 0.00100000000000000002 |Train loss: 0.59499532|Val loss: 0.56525891\n",
      "Epoch: 159 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.61458415\n",
      "Epoch: 160 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.58670647\n",
      "Epoch: 161 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.59202070\n",
      "Epoch: 162 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.64357926\n",
      "Epoch: 163 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.75111114\n",
      "Epoch: 164 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.75819920\n",
      "Epoch: 165 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.71541006\n",
      "Epoch: 166 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.72193487\n",
      "Epoch: 167 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70246528\n",
      "Epoch: 168 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.72314906\n",
      "Epoch: 169 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70447589\n",
      "Epoch: 170 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70157215\n",
      "Epoch: 171 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.72663760\n",
      "Epoch: 172 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70667564\n",
      "Epoch: 173 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.75442869\n",
      "Epoch: 174 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70155330\n",
      "Epoch: 175 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.73252092\n",
      "Epoch: 176 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70774557\n",
      "Epoch: 177 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.68280451\n",
      "Epoch: 178 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.70673739\n",
      "Epoch: 179 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.73262512\n",
      "Epoch: 180 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.76469377\n",
      "Epoch: 181 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.69958580\n",
      "Epoch: 182 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.75427867\n",
      "Epoch: 183 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.69657698\n",
      "Epoch: 184 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.67378878\n",
      "Epoch: 185 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.67701530\n",
      "Epoch: 186 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.65013783\n",
      "Epoch: 187 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.65437517\n",
      "Epoch: 188 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.67338580\n",
      "Epoch: 189 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.68390168\n",
      "Epoch: 190 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.66001959\n",
      "Epoch: 191 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.62981508\n",
      "Epoch: 192 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.63509262\n",
      "Epoch: 193 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.67282397\n",
      "Epoch: 194 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.64031494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.62597723\n",
      "Epoch: 196 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.63560965\n",
      "Epoch: 197 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.62675392\n",
      "Epoch: 198 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.60685668\n",
      "Epoch: 199 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.65037736\n",
      "Epoch: 200 | Lr: 0.00100000000000000002 |Train loss: 0.58925127|Val loss: 0.59036029\n",
      "\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm_notebook(range(1, epochs + 1)):\n",
    "    mean_loss, n = 0.0 , train_features.shape[1]\n",
    "    train_mean_loss, test_mean_loss = 0, 0\n",
    "    for i in range(n):\n",
    "      # Tensor.cpu()\n",
    "        train, train_target = get_features(totensor(train_features), i)\n",
    "        train, train_target = totensor(train), totensor(train_target)\n",
    "        val, val_target = get_features(totensor(val_features), i)\n",
    "        val, val_target = totensor(val), totensor(val_target)\n",
    "        y_train, y_val = model(train), model(val)\n",
    "        train_loss, val_loss = loss(y_train, train_target), loss(y_val, val_target)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.loc[i,'Training Loss'], losses.loc[i,'Validating Loss'] = train_loss.detach().cpu().numpy(), val_loss.detach().cpu().numpy()\n",
    "    train_loss, val_loss = losses['Training Loss'].mean(), losses['Validating Loss'].mean()\n",
    "    mean_losses.append((train_loss, val_loss))\n",
    "    if train_loss < min_val_loss:\n",
    "        min_val_loss = train_loss\n",
    "    print('Epoch: {:03d} | Lr: {:.20f} |Train loss: {:.8f}|Val loss: {:.8f}'.\\\n",
    "          format(epoch, optimizer.param_groups[0]['lr'], min_val_loss, val_loss))\n",
    "\n",
    "print('\\nTraining finished.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6eb055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
